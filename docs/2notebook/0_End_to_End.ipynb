{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e84g1YoseoE"
   },
   "source": [
    "# TextAttack End-to-End\n",
    "\n",
    "This tutorial provides a broad end-to-end overview of training, evaluating, and attacking a model using TextAttack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGv59SZzseoG"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/0_End_to_End.ipynb)\n",
    "\n",
    "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/0_End_to_End.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQTkpf9RslEA",
    "outputId": "20172655-e6ce-46c0-ba57-cb3a7f955ee3"
   },
   "source": [
    "!pip3 install textattack[tensorflow]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONayD5EJseoG"
   },
   "source": [
    "## Training\n",
    "\n",
    "First, we're going to train a model. TextAttack integrates directly with [transformers](https://github.com/huggingface/transformers/) and [datasets](https://github.com/huggingface/datasets) to train any of the `transformers` pre-trained models on datasets from `datasets`. \n",
    "\n",
    "Let's use the Rotten Tomatoes Movie Review dataset: it's relatively short , and showcasesthe key features of `textattack train`. Let's take a look at the dataset using `textattack peek-dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spS2eW5WseoG",
    "outputId": "795de4af-18b2-4750-d817-a03959c4cd25"
   },
   "source": [
    "!textattack peek-dataset --dataset-from-huggingface rotten_tomatoes"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uguqpjnLseoI"
   },
   "source": [
    "The dataset looks good! It's lowercased already, so we'll make sure our model is uncased. The longest input is 51 words, so we can cap our maximum sequence length (`--model-max-length`) at 64.\n",
    "\n",
    "We'll train [`distilbert-base-uncased`](https://huggingface.co/transformers/model_doc/distilbert.html), since it's a relatively small model, and a good example of how we integrate with `transformers`.\n",
    "\n",
    "So we have our command:\n",
    "\n",
    "```bash\n",
    "textattack train                      \\ # Train a model with TextAttack\n",
    "    --model distilbert-base-uncased   \\ # Using distilbert, uncased version, from `transformers`\n",
    "    --dataset rotten_tomatoes         \\ # On the Rotten Tomatoes dataset\n",
    "    --model-num-labels 3              \\ # That has 2 labels\n",
    "    --model-max-length 64             \\ # With a maximum sequence length of 64\n",
    "    --per-device-train-batch-size 128 \\ # And batch size of 128\n",
    "    --num-epochs 3                    \\ # For 3 epochs \n",
    "```\n",
    "\n",
    "Now let's run it (please remember to use GPU if you have access):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BY33W9aWseoI",
    "outputId": "0b0ec80a-6cec-4113-8474-b5bd78651b6c"
   },
   "source": [
    "!textattack train --model-name-or-path distilbert-base-uncased --dataset rotten_tomatoes --model-num-labels 2 --model-max-length 64 --per-device-train-batch-size 128 --num-epochs 3"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xzv3BGLseoI"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We successfully fine-tuned `distilbert-base-cased` for 3 epochs. Now let's evaluate it using `textattack eval`. This is as simple as providing the path to the pretrained model (that you just obtain from running the above command!) to `--model`, along with the number of evaluation samples. `textattack eval` will automatically load the evaluation data from training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGYR_W6DseoJ",
    "outputId": "a4edf6d3-9ac5-4513-ea26-754b409d5847"
   },
   "source": [
    "!textattack eval --num-examples 1000 --model ./outputs/2021-10-13-17-37-27-247436/best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFPkCZShseoJ"
   },
   "source": [
    "Awesome -- we were able to train a model up to 84.9% accuracy on the test dataset â€“ with only a single command!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWglEuvUseoK"
   },
   "source": [
    "## Attack\n",
    "\n",
    "Finally, let's attack our pre-trained model. We can do this the same way as before (by providing the path to the pretrained model to `--model`). For our attack, let's use the \"TextFooler\" attack recipe, from the paper [\"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\" (Jin et al, 2019)](https://arxiv.org/abs/1907.11932). We can do this by passing `--recipe textfooler` to `textattack attack`.\n",
    "\n",
    "> *Warning*: We're printing out 100 examples and, if the attack succeeds, their perturbations. The output of this command is going to be quite long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vL-Bo1bgseoK",
    "outputId": "aad8a4f1-bda7-4687-c79c-736201a29261"
   },
   "source": [
    "!textattack attack --recipe textfooler --num-examples 100 --model ./outputs/2021-10-13-17-37-27-247436/best_model/ --dataset-from-huggingface rotten_tomatoes --dataset-split test"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyrJM3CaseoL"
   },
   "source": [
    "Looks like our model was 84% successful (makes sense - same evaluation set as `textattack eval`!), meaning that TextAttack attacked the model with 84 examples (since the attack won't run if an example is originally mispredicted). The attack success rate was 98.8%, meaning that TextFooler failed to find an adversarial example only 1.2% (1 out of 84) of the time.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's all, folks! We've learned how to train, evaluate, and attack a model with TextAttack, using only three commands! ðŸ˜€\n",
    "\n",
    "\n",
    "\n",
    "## Bonus\n",
    "\n",
    "There are many powerful functions in TextAttack, we can use through command lines. Here is a list of examples as bonus for your learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!textattack attack --recipe deepwordbug --model lstm-mr --num-examples 2 --log-summary-to-json attack_summary.json"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!textattack attack --model cnn-yelp --num-examples 3 --search-method greedy-word-wir --transformation word-swap-wordnet --constraints cola^max_diff=0.1 bert-score^min_bert_score=0.7 --enable-advance-metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!textattack attack --model lstm-mr --recipe deepwordbug --num-examples 2 --attack-n --enable-advance-metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!textattack attack --model lstm-mr --recipe hotflip --num-examples 4 --num-examples-offset 3 --enable-advance-metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!textattack attack --model-from-huggingface distilbert-base-uncased-finetuned-sst-2-english --dataset-from-huggingface glue^sst2^train --recipe deepwordbug --num-examples 3 --enable-advance-metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "! textattack attack --model cnn-imdb --attack-from-file tests/sample_inputs/attack_from_file.py^Attack --num-examples 2  --num-examples-offset 18 --attack-n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "0_End_to_End.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
